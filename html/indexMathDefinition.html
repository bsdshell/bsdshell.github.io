<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-GB">
<head>
	<title>Surface, Curve, Algorithm, Geometry, Algebra</title>
	<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8" />
	<meta name="description" content="Surface, Curve, Algorithm, Geometry, Algebra" />
	<meta name="keywords" content="Surface, Curve, Algorithm, Geometry, Algebra" />
	<meta name="robots" content="index, follow" />
	<link rel="shortcut icon" href="/favicon.ico" type="../image/x-icon" />
	<link rel="stylesheet" type="text/css" href="../screen.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="../highlight.css">
    <link rel="stylesheet" type="text/css" href="../style.css">
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <script type="text/x-mathjax-config">
          MathJax.Hub.Config({
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX","output/HTML-CSS"],
            tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
          });
        </script>
    <script type="text/javascript" src="../MathJax-2.6-latest/MathJax.js"></script>
</head>
<body>
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-66282017-2', 'auto');
  ga('send', 'pageview');
</script>


<div id="header">
</div>
            <div class="mathdef_box2">
                \[\textbf{Ring}\]
                $\text{Let a, b, c } \in \mathbf{R}$
                $\text{There are two binary operations: addition and multiplication. They satisfy}$<br>
                \[ \text{Associative Law} \]
                \[ a \times b \times c = a \times (b \times c) \] 

                \[ \text{Distritutive Law} \]
                \[a \times (b + c) = a \times b + a \times c \]

                \[\text{Additive inverse} \]
                \[\text{For all a in $\mathbf{R}$, there exists b such that}\]
                \[a + b = 0 \]

                \[ \text{Multiplicative identity} \]
                \[ \forall a \in \mathbb{R}, \quad \exists 1 \in \mathbb{R}\text{ such as }\]
                \[1 \times a = a \times 1\]
            </div><br>
            <div class="mathdef_box2">
                \[\textbf{Ideal}\]
                Let $(\mathbf{I}, +)$ is the subgroup of $(\mathbf{R}, +)$
                Let $a \in \mathbf{I}$ and $r \in \mathbf{R}$, then $a \cdot r, r \cdot a \in \mathbf{I}$, $\mathbf{I}$ is called ideal of $\mathbf{R}$ <br>
                For example: <br>
                $2\mathbb{Z}$ is the ideal of $\mathbb{Z}$ <br><br>
                Proof: <br>
                \[
                    \text{let } a \in \mathbb{Z}, r = 2k \in 2\mathbb{Z} \quad \text{ where } a, k \in \mathbb{Z} \\ 
                    a \cdot r, r \cdot a = 2a \cdot k \in 2\mathbb{Z} \\ 
                    \implies 2\mathbb{Z} \text{ is ideal of } \mathbb{Z} \\ 
                \]
                Following screenshot can represent the ring $\mathbf{R} = (\mathbb{Z}, \mathbb{Z})$ and and ideal $\mathbf{I} = (2\mathbb{Z}, 2\mathbb{Z})$ blue dots
                <div class="cen">
                    <img src="../image/ring_ideal.png" width="40%" height="40%" /><br>  
                </div>
                
            </div><br>
           <div class="mathdef_box2">
           <div class="mytitle">
           Integral Domain 
           </div>
           Integral domain is commutative \textbb{Ring} $R$, if $a, b \in R$ and $a, b \neq 0$ then $ab \neq 0$
            </div>

           <div class="mathdef_box2">
           <div class="mytitle">
           Euclidean Domain 
           </div>
            An integral domain is called Euclidean if there exists function $f: R\backslash\{0\} \rightarrow \mathbb{N} \text{ satisfies the two properties:}$ <br> 
            1. $f(a) < f(ab) \text{ for all nonzero } a, b \in R$ <br> 
            2. $\forall a, b \in R \text{ with } b \neq 0 , \text{ there exists } q, r \text{ such that } a = q*b + r \text{ where } f(r) < f(q)$ <br> 
            </div>

            <div class="mathdef_box2">
                \[ \textbf{Ring homomorphism} \]
                Let $\phi$ is a function between two rings $R$, then $\phi$ is a $\mathit{ring}$ homomorphism if<br>
                for all $a \in R$ and $b \in R$<br>
                \[\phi(a+b) = \phi(a) + \phi(b)\] 
                \[\phi(ab) = \phi(a)\phi(b)\]
                and \[\phi(1) = 1\]

                Ideal<br>
                Let $R$ be a ring and let $I$ is additive subgroup of $R$, then $I$ is called an ideal of $R$ and write $I \triangleleft R$<br>
                if $\forall a \in I$ and $\forall r \in R $, and $ ar \in I$ and $ra \in I$<br><br>
                Example<br>
                $R = (\mathbb{N}, +)$ and $I = (2k, +) \quad k \in \mathbb{N}$<br>

                \[\text{Let I be a kernal of } \phi, \text{ then I is an ideal of R} \]
                Let $a \in I$ and $r \in R$, then $\phi(ra) = \phi(r)\phi(a)$<br>
                $I$ is kernal of $\phi$<br>
                $\Rightarrow \phi(a) = 0 \therefore \phi(ra) = 0, \therefore ra \in I$<br>

                Let $\phi: \mathbb{C} \rightarrow \mathbb{C}$ be the map send a complex number to its complex conjugate. 
                Then $\phi$ is an automorphism of $\mathbb{C}$. $\phi$ is its own inverse.<br><br>

                \begin{equation}
                \begin{aligned}
                \phi(z) &= \overline{z}\\\\
                \phi(z_1 + z_2) &= \overline{z_1 + z_2}\\
                \overline{z_1 + z_2} &= \overline{z_1} + \overline{z_2}\\
                \phi(z_1 z_2) &= \overline{z_1 z_2}\\
                \overline{z_1 z_2} &= \overline{z_1} \cdot \overline{z_2} \nonumber\\
                \phi(\phi(z)) &= z<br>
                \end{aligned}
                \end{equation}
                <img src="../image/conjugate.svg" width="100%" height="100%" /><br>  

                Let $\phi: \mathbb{R}[x] \rightarrow  \mathbb{R}[x]$ be the map that send $f(x)$ to $f(x+1)$.<br> 
                Then $\phi$ is an automorphism of \mathbb{R}[x]. The inverse map sends $f(x)$ to $f(x-1)$<br>

            </div>
            <br>

            <div class="mathdef_box2">
                \[ \textbf{ Semigroup } \]
                Semigroup is a set $S$ and a binary operator $\otimes \colon S \times S \rightarrow S$ that satisfies 
                associative property<br> 
                \[ \forall \text{ a, b, c} \in S \text{ such as } a \otimes b\otimes c = a \otimes (b \otimes c) \]
            </div><br>

            <div class="mathdef_box2">
                \[ \textbf{ Monoid } \]
                A monoid is a triple $(S, \otimes, \overline{1})$<br> 
                1. $\otimes$ is closed associative binary operator on the set $S$<br> 
                2. $\overline{1}$ is identity element for $\otimes$<br> 
                $\forall\quad a, b, c \in S$ such as<br>
                \[ a \otimes b  \otimes c = a \otimes (b \otimes c)   \]
                \[ a \otimes \overline{1} = \overline{1} \otimes a =  a  \]
            </div><br>
            <div class="mathdef_box2">
<pre class="prettyprint">
abstract class SemiGroup[A]{
    def add(x: A, y:A):A
}

abstract class Monoid[A] extends SemiGroup[A]{
    def unit: A
}

object MyMonoid extends App{
      implicit object StringMonoid extends Monoid[String]{
          def add(x: String, y:String):String = x concat y
          def unit: String = ""
      }

      implicit object IntMonoid extends Monoid[Int]{
          def add(x: Int, y:Int):Int = x + y
          def unit: Int = 0
      }
      def sum[A](xs: List[A])(implicit m: Monoid[A]): A = {
          if(xs.isEmpty) m.unit 
          else m.add(xs.head, sum(xs.tail))
      }
      println(sum(List(1, 2, 3)))
      println(sum(List("a", "b", "c")))
}
</pre>
 
            </div><br>

            <div class="mathdef_box2">
               <div class="mytitle">
               Definition of Group
               </div>
                $\text{Let a, b, c} \in \mathbf{G}$<br>
                There is binary operation * and satisfy<br>
                Closure Law<br>
                $ a*b \in \mathbf{G} $<br>
                Associative Law<br>
                $ a*b*c = a*(b*c)$<br>
                Identity<br>
                $\exists \mathit{e} \in \mathbf{G} \text{ such that } \mathit{e}*a = a*\mathit{e} \in \mathbf{G}$<br>
                Inverse<br>
                $ \text{If a } \in \mathbf{G}, \exists a^{-1} \in \mathbf{G} \text{ such that } a*a^{-1} = e $<br>
            </div>
            <div class="mathdef_box2">
               <div class="mytitle">
               Definition of SubGroup
               </div>
                Given a group $(G, \otimes)$ <br>
                1. $H$ is the subset of $G$ <br>
                2. $H$ forms a group under the same binary operation as $G$ <br> 
                
                $(\mathbb{Z}, +)$<br>
                $(\mathbb{2Z}, +)$<br>
           </div>
            <div class="mathdef_box2">
               <div class="mytitle">
               Coset of a Group 
               </div>
               Coming soon
           </div>
            <div class="mathdef_box2">
               <div class="mytitle">
               Normal Subgroup
               </div>
               Coming soon
           </div>

            <div class="mathdef_box2">
            <div class="mytitle">
            Group homomorphism(operation preserving)
            </div>
                Given group $(G, \oplus)$ and $(H, \otimes)$,for all $a, b \in G$<br>
                If there is function $\phi : G \rightarrow H$ <br>
                If $\phi(a \oplus b) = \phi(a) \otimes \phi(b)$, then $\phi$ is group homomorphism from $(G, \oplus)$ to $(H, \otimes)$<br><br>
                <strong>Concreate example</strong><br>
                Given $G(\mathbb{R}, +)$ and $H(\mathbb{R}, *)$, then $\phi(x) = e^x$ is homomorphism from $G(\mathbb{R}, +)$ to $H(\mathbb{R}, *)$<br>
                \begin{align*}
                \forall &a, b \in \mathbb{R} \\
                \phi(a + b) &= e^{a + b}  \\
                \phi(a)*\phi(b) &= e^{a}*e^{b} = e^{a+b} \\
                \Rightarrow \phi(a_1 + b_1) &= \phi(a_2)*\phi(b_2) \\
                \Rightarrow \phi(x) &= e^{x} \text{ is homomorphism from } G(\mathbb{R}, +) \text{ and } H(\mathbb{R}, *) \\
                \end{align*}
                Note:
                $\phi$ will map the identity of $G$ to identity of $H$ automatically, and we can derive it, Why Not?<br>
                \begin{align*}
                &\forall a, b \in \mathbb{R} \\
                &\text{Let a = 0, b = 0} \\
                &\phi(0 + 0) = e^{0 + 0} = 1 \\
                &\phi(0)*\phi(0) = e^{0}*e^{0} = 1*1 = 1 \\
                &\Rightarrow \phi(0_G) = 1_H \\
                &\Rightarrow \phi \text{ map the identity } 0_G \in G \text{ to the identity} 1_H \in H  \quad \square
                \end{align*}


            </div>
            <br>
            <div class="mathdef_box2">
                \[\textbf{Vector Space}\]
                $\text{Let }\vec{u}, \vec{v}, \vec{w} \in \vec{V} \text{ and scalars } \alpha, \beta \in \mathbb{F}$<br>
                Closure<br>
                $\vec{u} + \vec{v} \text{ and } \in \vec{V}$<br>
                Associative Law<br>
                $\vec{u} + \vec{v} + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$<br>
                Commutative Law<br>
                $\vec{u} + \vec{v} = \vec{v} + \vec{u} $<br>
                Identity element of addition<br>
                $\exists \vec{0} \in \vec{V} \text{ such that } \forall \vec{u} \in \mathbb{V}$<br>
                Inverse element of addition<br>
                $\exists -\vec{u} \text{ such that } \vec{u} + (-\vec{u}) = \vec{0}$<br>
                Identity element of scalar multiplication<br>
                $\exists \mathit{1} \in \mathbb{F} \text{ such that } \mathit{1}\vec{u} = \vec{u}$<br>
                Distributivity of scale multiplication with respect to vector addition<br>
                $\alpha(\vec{u} + \vec{v}) = \alpha\vec{u} + \alpha\vec{v}$<br>
                Distributivity of scale multiplication with respect to field addition<br>
                $(\alpha + \beta)\vec{u} = \alpha\vec{u} + \beta\vec{u}$<br>
            </div>

            <br>
            <div class="mathdef_box2">
            \[ \textbf{Linear Transformations} \]
            \begin{aligned}
                & \mbox{A function } \mathit{T}: \mathbb{R}^n \rightarrow \mathbb{R}^m \mbox{ is called linear transformation, if it satisfies} \\
                & \mathit{T} ( \mathbf{u} + \mathbf{v} ) = \mathit{T}(\mathbf{u}) + \mathit{T}(\mathbf{v}) \quad \forall \; \mathbf{u} \,, \mathbf{v} \in \mathbb{R}^n\\
                & \mathit{T} ( \lambda \mathbf{u} ) = \lambda \mathit{T}(\mathbf{u}) \quad \mbox{all scalars } \lambda \\
            \end{aligned}
            </div>
            <br>

            <div class="mathdef_box2">
                \[\textbf{Euclidean Space}\]
                Let $V \in \mathbb{R}^n$ is vector space and an inner product is a function $ \left< , \right>: V \times V \rightarrow \mathbb{R}$, that is 
                $(x, y) \rightarrow \left< x \,, y \right>$ for $x, y \in V$, which satisfied following axioms:
                \begin{equation}
                \begin{aligned}
                    \langle ax \,, y \rangle &= a \langle x \,, y \rangle \\  
                    \langle x \,, by \rangle &= b \langle x \,, y \rangle \\
                    \langle x + y \,, z \rangle &= \langle x \,, z \rangle + \langle y \,, z \rangle \\
                    \langle x\,, y + z \rangle &= \langle x \,, y \rangle + \langle x \,, z \rangle \\
                    \langle y \,, x \rangle &= \langle x \,, y \rangle \\
                    \langle x \,, x \rangle &> 0 \quad x \neq 0 \quad \text{positive definite}\\
                \end{aligned}
                \end{equation}
                If the inner product is defined as $ \left< u, v \right> = u^{T}v $    , we have Euclidean Structure $(\mathbb{R}^{n}, u^{T}v)$ or $(\mathbb{R}^{n}, \circ)$
            </div>
            <br>

            <div class="mathdef_box2">
                \[\textbf{Euclidean Structure}\]
                Euclidean Structure is defined by Inner product  
                \[ \langle \vec{u}, \vec{v}  \rangle = \sum_{k=1}^{n} u_{k} v_{k}\]
                Length function is defined by the norm of Inner product
                \[  \| \langle \vec{u}, \vec{u}  \rangle \|  = \sqrt{ \sum_{k=1}^{n} u_{k}^2 }\]
                Distance function is called Euclidean metric. The formula expresses a special case of Pythagorean Theorem.
                \[  d(u - v) = \| u - v \| = \sqrt{ \sum_{k=1}^{n} (u_{k}-v_{k})^2  }\]
                The angle between $\vec{u}$ and $\vec{u}$ is given by
                \[ \beta = \arccos \frac{ \langle \vec{u}, \vec{v} \rangle }{\|\vec{u}\| \|\vec{v}\| } \] 
            </div>
            <br>

            <div class="mathdef_box2">
            </div>
            <br>


            <div class="mathdef_box2">
                \[\textbf{Affine Space}\]
                An affine space is a set of points that admits free transitive action of a vector space $\vec{V}$ That is, there is a map $X \times \vec{V} \rightarrow X:(x, \vec{v}) \mapsto x + \vec{v}$,<br> called translation by a vector $\vec{v}$, such that<br>
                1. Addition of vectors corresponds to composition of translation, i.e., for all $x \in X \text{ and } \vec{u}, \vec{v} \in \vec{V}, (x + \vec{u}) + \vec{v} = x + (\vec{u} + \vec{v})$<br> 
                2. The zero vector $\vec{0}$ acts as the identity vector, i.e., for all $x \in X, x + \vec{0} = x$<br>
                3. The action is transitive, i.e., for all $x, y \in X, \text{ exists } \vec{v} \in \vec{V} \text{ such that } y = x + \vec{v}$<br>
                4. The dimension of X is the dimension of vector space translations, $\vec{V}$<br>
                <br>
                Or There is unique map<br>
                $X \times X \rightarrow \vec{V}:(x, y) \mapsto y - x \text{ such that } y = x + (y - x) \text{ for all }x, y \in X$<br>
                It furthermore satifies<br> 
                1. For all $x, y, z \in X, z - x = (z - y) + (y - x)$<br>
                2. For all $x, y, \in X$ and $\vec{u}, \vec{v} \in \vec{V}$, $ (y + \vec{v}) - (x + \vec{u}) = (y - x) + (\vec{v} - \vec{u})$<br>
                3. For all $x \in X, x - x = \vec{0}$<br>
                4. For all $x, y \in X, y - x = -(x - y)$<br>
            </div>
            <br>
            <div class="mathdef_box2">
                \[ \textbf{Affine Space from linear system equation} \]
                Consider an $(m \times n)$ linear sytem equations<br>
                $\sum_{k=1}^{n} a_{i k} x_{k} = c_{i}, (1 \leq i \leq m) \quad\quad\quad \text{(1)}$<br>
                where $d = n - rank(M), c_{i} \ne \vec{0} \in \mathbb{R}^{m}$<br>
                When the system has at least one solution $x_{p}$ then the full set of solution is a d-dimension affine space<br>
                $A \subset \mathbb{R}^{n}$<br> 
                Since $x_{p} \in A, \text{ we can declare point } x_{p} \text{ as origin of A and then introduct A coordinates as follows:homogenous system}$<br>

                $\sum_{k=1}^{n} a_{i k} x_{k} = \vec{0} \quad (1 \leq i \leq m)$<br>
                $\Rightarrow dim(\ker(M)) = d \quad \text{(Rank Theorem)}$<br>
                $\Rightarrow \text{(1) has d-linear independent solution } \vec{b_{j}} \in \mathbb{R}^{n} \quad\quad (1 \leq j \leq d)$<br>
                Affine Space $A$ can be written as<br> 
                $A = \Big\{ x_{p} + \sum_{j=1}^{d}\alpha_{j}\vec{b_{j}} \quad \mid \quad \alpha_{j} \in \mathbb{R} \quad\quad (1 \leq j \leq d)\Big\} $<br>
                $\text{The } \alpha_{j} \text{ can be served as coordinates in A, so that A looks as it were a d-dimension coordiate space.}$<br>
                $\text{But note that addition(+) in the space refers to the chosen point } x_{p}, \text{ and not to the origin of the base vector space}$<br>
            </div>
            <br>


            <div class="mathdef_box2">
                \[ \textbf{Affine space and linear system} \]
                The solution set $\mathit{K}$ of any system $\mathbf{A}\mathbf{x}=\mathbf{b}$ of $m$ linear equations in $n$ unknowns is 
                an affine space, namely a coset of $\ker{T_{A}}$ represented by a particular solution $\mathbf{s} \in \mathbb{R}^{n}$ 
                \[ \mathit{K} \in \mathbf{s} + \ker{T_{A}}  \]
                $\mathbf{Proof}$: If $\mathbf{s} \,, \mathbf{w} \in \mathbf{K}$, then 
                $\mathbf{A}(\mathbf{s} - \mathbf{w}) = \mathbf{A}\mathbf{s} - \mathbf{A}\mathbf{w} = \mathbf{b} - \mathbf{b} = \mathbf{0}$
                so that $\mathbf{s} - \mathbf{w} \in \ker{T_{A}}$. Now let $\mathbf{k} = \mathbf{s} - \mathbf{w} \in \ker{T_{A}}$. Then
                \[ \mathbf{w} = \mathbf{s} + \mathbf{k} \in \ker{T_{A}} \]
                Hence $\mathbf{K} \subseteq \mathbf{s} + \ker{T_{A}}$. To show the conversion inclusion, suppose $\mathbf{w} \in \mathbf{s} + \ker{T_{A}}$. Then $\mathbf{w} = \mathbf{s} + \mathbf{K}$ for some $\mathbf{k} \in \ker{T_{A}}$. 
                But then 
                \[ \mathbf{A}\mathbf{w} = \mathbf{A}(\mathbf{s} + \mathbf{k}) = \mathbf{A}\mathbf{s} + \mathbf{A}\mathbf{k} = \mathbf{b} + \mathbf{0} = \mathbf{b} \]
                so $\mathbf{w} \in \mathit{K}$, and $\mathbf{s} + \ker{T_{A}} \subseteq \mathit{K}$. Thus, $\mathit{K} = \mathbf{s} + \ker{T_{A}} \quad \square$
            </div>
            <br>

            <div class="mathdef_box2">
                \[\textbf{If }  \gcd(a, b) = 1 \textbf{ and }  a \vert bc\]
                \[ \textbf{Prove } a \vert c \]
                $\gcd(a, b) = 1  $<br>
                $\Rightarrow \exists m, n \in \mathbf{N} \quad ma+nb = 1$<br>
                $\Rightarrow mac + nbc = c$<br> 
                $\Rightarrow ak = bc \quad k \in \mathbf{N} \because a \vert bc \quad $<br>
                $\Rightarrow mac + n(ak)=c \quad    (ak=bc) $<br> 
                $\Rightarrow a(mc + nk) = c$<br>  
                $\Rightarrow a \vert c $<br> 
            </div>
            <br>
            <div class="mathdef_box2">
                \[\textbf{Theorem 1}\] 
                The image of transformation is spanned by the image of the any basis of its domain.<br> 
                For $T:\vec{V} \rightarrow \vec{W}, \text{ if } \beta=\{ \vec{b_1},\vec{b_2},...,\vec{b_n} \} \text{ is a basis of }\vec{V}, \text{ then }T(\beta) = \{ T(\vec{b_1}), T(\vec{b_2}), ... ,T(\vec{b_n})\} \text{ spans the image of }T$<br>
                <br>
                For all $\vec{v} \in \vec{V}, \vec{v} = \alpha_1\vec{b_1} + \alpha_2\vec{b_2} + ... + \alpha_n\vec{b_n}$<br>
                $\Rightarrow T(\vec{v}) = T(\alpha_1\vec{b_1} + \alpha_2\vec{b_2} + ... + \alpha_n\vec{b_n})$<br>
                $\Rightarrow T(\vec{v}) = \alpha_1 T(\vec{b_1}) + \alpha_2 T(\vec{b_2}) + ... + \alpha_n T(\vec{b_n})$<br>
                $\Rightarrow \{ T(\vec{b_1}), T(\vec{b_2}),...,T(\vec{b_n})\} \text{ spans the image of }T$<br>
            </div>
            <br>
            <div class="mathdef_box2">
                \[\textbf{Rank Theorem} \] 
                If the domain is finite dimension, then the dimension of domain is the sum of rank and nullity of the transformation<br>

                $\text{Let } T:\vec{V} \rightarrow \vec{W} \text{ be a linear transformation },\text{let n be the dimension of }\vec{V},$<br>
                $\text{let k be nullity of }T \text{ and let k be the rank of }T$<br>
                $\text{Show } n = k + r$<br>

                $\text{Let }\beta = \{ \vec{b_1}, \vec{b_2},...,\vec{b_k}\} \text{ be the basis of kernal of }T, \text{ the basis can be extended to } \gamma = \{ \vec{b_1}, \vec{b_2},...,\vec{b_k}, \vec{b_{k+1}},...,\vec{b_n}\}$<br>
                $\text{let }\vec{v} \in \vec{V} \Rightarrow \vec{v} = \alpha_1 \vec{b_1} + \alpha_2 + \vec{b_2} +,..., + \alpha_k \vec{b_k} + \alpha_{k+1}\vec{b}_{k+1}+,...,+\alpha_{n}\vec{b_n}$<br>
                $\text{Let }T(\vec{v}) = T(\alpha_1 \vec{b_1} + \alpha_2 + \vec{b_2} +,..., + \alpha_k \vec{b_k} + \alpha_{k+1}\vec{b}_{k+1}+,...,+\alpha_{n}\vec{b_n}) = \vec{0}$<br>
                $\Rightarrow \vec{v} = \alpha_1 \vec{b_1} + \alpha_2 + \vec{b_2} +,..., + \alpha_k \vec{b_k} + \alpha_{k+1}\vec{b}_{k+1}+,...,+\alpha_{n}\vec{b_n} \in \ker(T) \quad\quad \text{(1)}$<br>
                $\because \vec{v} = \sigma_1 \vec{b_1} + \sigma_2 + \vec{b_2} +,..., + \sigma_k \vec{b_k} \in \ker(T) \quad\quad \text{(2)}$<br>
                $(1) - (2) \Rightarrow \vec{0} = (\alpha_1-\sigma_1)\vec{b_1} + (\alpha_2 - \sigma_2)\vec{b_2}+,...,+ (\alpha_k - \sigma_k)\vec{b_k}+   \alpha_{k+1}\vec{b}_{k+1}+,...,+\alpha_{n}\vec{b_n} $<br>
                $\because \vec{b}_{1}, \vec{b}_{2},...,\vec{b}_{k},\vec{b}_{k+1}, \vec{b}_{k+2},...,\vec{b_n} \text{ are linearly independent}$<br>
                $\therefore \alpha_{k+1}, \alpha_{k+2}, ... , \alpha_{n} \text{ are all zero} \quad\quad \text{(3)}$<br>
                $T(\vec{v}) = T(\alpha_1 \vec{b_1}) + T(\alpha_2 \vec{b_2}) +,..., + T(\alpha_k \vec{b_k}) + T(\alpha_{k+1}\vec{b}_{k+1})+,...,+T(\alpha_{n}\vec{b_n}) = \vec{0}$<br>
                $T(\vec{v}) = \alpha_1 T(\vec{b_1}) + \alpha_2 T(\vec{b_2}) +,..., + \alpha_k T(\vec{b_k}) + \alpha_{k+1}T(\vec{b}_{k+1})+,...,+\alpha_{n}T(\vec{b_n}) = \vec{0}$<br>
                $\because \beta = \{ \vec{b_1}, \vec{b_2},...,\vec{b_k}\} \text{ is the basis of kernal of }T$<br>
                $\therefore T(\vec{b_1}) = \vec{0},..., T(\vec{b_k}) = \vec{0}$<br>
                $\therefore T(\vec{v}) = \alpha_{k+1}T(\vec{b}_{k+1})+,...,+\alpha_{n}T(\vec{b_n}) = \vec{0} \quad\quad \text{(4)}$<br>
                $\text{(3) and (4)} \Rightarrow \{ T(\vec{b}_{k+1}), T(\vec{b_{k+2}}), ... , T(\vec{b_{n}}) \} \text{ are linearly independent}$<br>
                $\Rightarrow \dim(\vec{V}) = \text{ nullity(T) } + \text{ rank(T) } \text{ or }$<br>
                $\Rightarrow \dim(\vec{V}) = \dim(\ker(T)) + \dim(\text{img(T)}) $<br>
                $\Rightarrow n = k + r \quad \square$<br>
            </div>
            <br>

            <div class="mathdef_box2">
            <div class="cen">
            <img src="../image/ranktheorem.png" width="60%" height="60%" /><br> 
            </div>
            </div>
            
            <div class="mathdef_box2">
            \[\textbf{Chart}\]
            A $\textbf{Chart}$ on a set $M$ is a pair $(\phi, U)$ where $U$ is an open subset of $M$ and $\phi: U \rightarrow \phi(U)$ is bijection from $U$ to an open subset $\phi(U)$ in $\mathbb{R}^{m}$
            An $\textbf{Atlas}$ is collection of $\mathscr{A} = \{(\phi, U)\}_{\alpha \in A}$ of charts such that the domains $U_{\alpha}$ conver $M$
            \[
                M = \bigcup_{\alpha \in A} U_{\alpha}
            \]

            Example:<br>
            Every open subset $U \in M$ has an Altas consisting of a single chart $(\phi, U) = (id_{U}, U)$, where $id_{U}$ denoted identity map of $U$ 
            </div>

            <div class="mathdef_box2">
                \[\textbf{Topological Space}\]
                $\textbf{A topological space}$ is pair $(X, T)$ where $X$ is a set and $U$ is subset of $X$ satifying certain axioms. $T$ is called topology<br>
                1. $\emptyset \in T$ and space $X \in T$<br>
                2. If $U_1 \in T, U_2 \in T$, then $U_1 \cap U_2 \in T$ $\textbf{ finite}$<br>
                3. If $U_i \in T$ then $\bigcup_{i \in I} U_i \in T$ $\textbf{ finite or infinite}$<br><br>

                \[ 
                    X = \{1, 2\} \\
                    T = \{ \emptyset, \{1\}, \{2\}, \{1, 2\}\}
                \]
                $(X, T)$ is topological space becase it satisfied three axioms <br><br> 

                The elements of $T$ is called open sets,<br>
                Property 2 implies any $\mathbf{finite}$ intersection of open sets is open<br>
                Property 3 implies union of any open sets is open<br>
                Any collection of subsets of $X$ satifies above properties is called $\textbf{topology}$ on $X$<br>

                \[ \textbf{Topology} \]
                $\text{Let }\mathcal{M} \text{ be a set. A topology }\mathcal{Q} \text{ is a subset } \mathcal{Q} \subseteq \mathcal{P}(\mathcal{M})$ Satisfy<br> 
                $1. \varnothing\subseteq \mathcal{Q}, \mathcal{M} \subseteq \mathcal{Q}$<br>
                $2. \mathcal{U} \subseteq \mathcal{Q},   \mathcal{V} \subseteq \mathcal{Q} \implies \mathcal{U} \cap \mathcal{V} \in \mathcal{Q}$<br>
                $3. \mathcal{U} \in \mathcal{Q} \implies \bigcup_{\alpha \in \mathcal{A}} \mathcal{U}_\alpha \in \mathcal{Q}$<br>
                <div class="cen">
                <img src="../image/topologicspace.png" width="50%" height="50%" /><br> 
                </div>
            </div>
            <div class="mathdef_box2">
                <div class="mytitle">
                Homeomorphism - deformation, stretches, push around, shrink, enlarge without rips
                </div>
                
                A function $f:X \rightarrow Y$ between two topological spaces $(X, T_x)$ and $(Y, T_y)$ is called $\textbf{Homeomorphism}$ if it has the following properties:<br>
                1. $f$ is bijective<br>
                2. $f$ and $f^{-1}$ are both continuous<br>
                Take the donus to coffee cup, you can stretch and push around without rips.<br>
                Continuous and Invertable funciton from one topological space to other topoligical space.
                <div class="cen">
                <img src="../image/homeomorphism1.png" width="50%" height="50%" /><br> 
                </div>
                \[\textbf{Smooth}\]
                Given $f: U \rightarrow V$ $f$ is continuous in $U$ and  
                all derivatives of the $f_i$ of any orders exist <br><br>
                \[\textbf{Diffeomorphic}\]
                1. $f$ is Homeomorphic <br>
                2. $f$ and $f^{-1}$ both are differentiable in any order.(smooth or $C^{\infty}$)<br><br>

                Sequenently, for every $u \in \tau_x$, there is $v \in \tau_y$ such that $v = f(u)$ and $u = f^{-1}(v)$<br>
                Furthere more, since
                \[
                    f(u_1 \cap u_2) = f(u_1) \cap f(u_2) \\ 
                    f(u_1 \cup u_2) = f(u_1) \cup f(u_2)
                \]
                the equavalence extends to the structures of the spaces

            </div>

            <div class="mathdef_box2">
                <div class="cen">
                <img src="../image/morphic.svg" width="80%" height="80%" /><br> 
                </div>
            </div>

            <div class="mathdef_box2">
            Injective <br>
            $\text{if } f(x_1) = y_2 \text{ and } f(x_2) = y_2 \text{ and } y_1 = y_2, \text{ then } x_1 = x_2 $ <br><br>
            Surjective <br>
            $\forall y \in Y \quad \exists x \in X  \text{ such as } f(x) = y $ <br><br>
            Bijective <br>
            $f \text{ is injective and surjective} $ 
            </div>


            <div class="mathdef_box2">
            <div class="mytitle">
            Voronoi Diagram 
	        </div>                                                                             
            Definition of Voronoi Diagram<br>
            For $p, q \in S$ let 
            \[
                B(p, q) = \{ x \mid d(p, x) < d(p, x) \}
            \]
            be the bisector of $p, q$, $B(p, q)$ is perpendicular to line through the center of line segment $\overline{pq}$<br>

                Given a set $S$ of n points in a plane, we wish to associate with each point $s$ a region consisting of all points in the plane closer to $s$
                than any other point $s'$ in $S$. The can be described formally as
                \[ 
                    \mathbf{Vor}(\mathbf{s}) = \{p: \textbf{dist}(s, p) \leq \textbf{dist}(s', p), \forall s' \in S \}
                \]
                Where $\mathbf{Vor}(\mathbf{s})$ is the Voronoi region for a point $s$<br><br>

            Two points Voronoi Diagram<br>
            <img src="../../image/voronoi_diagram1.svg" width="80%" height="80%" /><br> 
            Three points Voronoi Diagram<br>
            <img src="../../image/voronoi_diagram.svg" width="80%" height="80%" /><br> 
            </div>

            <div class="mathdef_box2">

            Differential, Forwad Differentiation, can be implemented in Haskell
            \begin{align*}
            (x + \varepsilon x') + (y + \varepsilon y') &= (x + y) + \varepsilon(x' + y') \\
            (x + \varepsilon x')(y + \varepsilon y') &= xy + \varepsilon(x'y + y'x) \\
            f(x + \varepsilon x') &= f(x) + \varepsilon f'(x)x' \\
            f(g(x + \varepsilon x')) &= f(g(x + \varepsilon x')) + \varepsilon f'(g(x + \varepsilon x'))  \\ 
            f(g(x + \varepsilon x')) &= f( g(x) + \varepsilon g'(x) x')  \\
            f(g(x + \varepsilon x')) &= f(g(x)) + \varepsilon f'(g(x)) g'(x)x'   \\
            \end{align*}
            </div>

            <div class="mathdef_box2">

            <div class="mytitle">
            Mean Value Theorem in Calculus<br>
	        </div>                                                                             

            If $f$ is continuous on closed interval $[a, b]$ and differential on open interval $(a, b)$, then there exists point $c$ such that
            \begin{align*}
                f'(c) = \frac{f(b) - f(a)}{b - a} 
            \end{align*}

            <div class="cen">
            <img src="../image/mean_value_theorem.svg" width="60%" height="60%" /><br>
            </div> 
            </div>
            

            <div class="mathdef_box2">
            <div class="mytitle">
            What is Algebra<br>
	        </div>                                                                             

            We all talk about Algebra from high school to University, but we hardly were given any defintion of Algebra.<br>
            This is the defintion from Hopf Lecture Notes<br><br>
            An algebra $A$ can be defined as triple $A=(V, *, I)$ where $V$ is Vector Space, $*$ is multiplication and $I$ is identity<br>
            Given Vector Space $V$ over the field $K$, e.g. $K = \mathbb{C}$, <strong>Algebra</strong> satisfies following properties:<br>
            \begin{align*}
                &a, b, c \in V \\
                &*: V \times V \rightarrow V \quad \text{Bilinearity} \tag{1}\\
                &a * b * c = a * (b * c) \quad \text{Associativity}  \\
                &1*a = a*1               \quad \text{Unitality} \\
            \end{align*}<br>

            Example: <strong>n dimension matrix</strong> over the field $K = \mathbb{C}$ <br>
            It is easy to show all the matrices form a <strong>Vector Space</strong> since it satisfied all the Vector Space properties<br>
            \begin{align*}
            &\forall m_1, m_2, m_3 \in V \\
            &m_1 + m_2 + m_3 = m_1 + (m_2 + m_3) \\
            &m_1 + 0 = 0 + m_1 \\
            &m_1 + m_2 = 0 \\
            &... \\
            &\text{ What is the basics of a Vector Space in two dimensions, e.g.} \\

            &A= \begin{bmatrix}
            1 & 2 \\
            3 & 4 \\
            \end{bmatrix} \\
            &\text{ The following is the basics of for two dimensions matrix } \\
            &\left\{
                e_1= \begin{bmatrix}
                1 & 0 \\
                0 & 0 \\
                \end{bmatrix},

                e_2= \begin{bmatrix}
                0 & 1 \\
                0 & 0 \\
                \end{bmatrix},

                e_3= \begin{bmatrix}
                0 & 0 \\
                1 & 0 \\
                \end{bmatrix},

                e_4= \begin{bmatrix}
                0 & 0 \\
                0 & 1 \\
                \end{bmatrix} 
            \right\} \\
            &\text{Any 2x2 matrix can be written in linear combination of } \left\{e_1, e_2, e_3, e_4 \right\} \\
            &A = 1e_1 + 2e_2 + 3e_3 + 4e_4 \\
            \end{align*}

            Once we have a Vector Space, We can define the <strong>multiplication</strong>$(*)$ from $*: A \times A \rightarrow A$ <br><br> 
            We know Inner Product is Bilinearity, let's see the meaning of Bilinearity for Inner Product<br>
            \begin{align*}
                &\forall u, v, w \in V, \alpha \in F, \text{ e.g. } F = \mathbb{R} \\
                &\left< , \right> : V \times V \rightarrow \mathbb{R} \quad \text{ definition of Inner Product} \\
                &\left< u, v \right> = u^{T}v \\ 
                &\left< u + v, w \right> = \left< u, w \right> + \left< v, w \right> \quad \text{ linear on the left} \\
                &\left< u, v + w \right> = \left< u, v \right> + \left< u, w \right> \quad \text{ linear on the right} \\
            \end{align*} 
            It is very similar to distributive law, e.g. <br>
            \begin{align*}
            a(b + c) &= ab + ac = (b + c)a \\ 
            \left< a, b + c \right> &= \left< a, b \right> + \left< a, c \right> \\
            \end{align*}
            

            The <strong>multiplication</strong> can be defined as matrix multiplication <br>
            \begin{align*}
            &*: V \times V \rightarrow V \\
            &*: M \times M \rightarrow M \quad \text{ where } V = M\\
            &*:(M, M) \rightarrow M \quad \text{Prefix notation}\\
            &M*M \rightarrow M      \quad \text{Infix notation} \\ 
            &\forall A, B, C \in M, \alpha \in \mathbb{F} \\
            &\text{Check the Associativity} \\
            &A * B * C = A * (B * C) \\
            &\text{Check the Bilinearity} \\
            &A(B + C) = A B + A C = (B + C)A \\
            &\text{Check Unitarity} \\
            &I A = A I \quad \text{ where $I$ is identity matrix}\\
            & \alpha A B = \alpha (AB) = A (\alpha B) = 
            \end{align*}
We did not need to check the multiplication of scalar and M since it is the property of Vector Space, not the Bilinear property <br><br>

           <div class="mytitle">
            Bilinear Form<br>
	       </div>                                                                             

Definition of Bilinear form on a <strong>Vector Space</strong> $V$ over the field $\mathbb{F}$ is a map
\[ 
    H: V \times V \rightarrow \mathbb{F}  \\
\] 
The map satifieds following properties: <br>
            \begin{align*}
                &\forall u, v, w \in V, \alpha \in \mathbb{F} \\
                &H(u + v, w) = H(u, w) + H(v, w) \\
                &H(u, v + w) = H(u, v) + H(u, w) \\
                &H( \alpha u, v) = \alpha H(u, v) \\
                &H( u, \alpha v) = \alpha H(u, v) \\
            \end{align*}
The $H$ is similar like $\times$ multiplication that we have known <br>
\begin{align*}
    H(w, u + v) &= H(w, u) + H(w, v) \\
    \times(w, u + v) &= \times(w, u) + \times(w, v) \\
    w \times (u + v) &= w \times u + w \times v \\ 
\end{align*}
What we have familiarized is Inner Product, but Inner Product is more specific<br>
\begin{align*}
    \left< u, u \right > \geq 0 \Leftrightarrow u = 0 \\
\end{align*}



           <div class="mytitle">
            Matrix bilinear form<br>
	       </div>                                                                             

Definition:
Let $g$ be a bilinear form on space $V$, and let $\mathcal{\beta} = \{b_1, b_2, ... , b_n \}$
be a basis of $V$. Then the Matrix $G = (g_{i,j}) = g(b_i, b_j)$ is called the matrix of bilinear form $g$
with respect to the basis $\beta$, we will call matrix $G$ a Gram matrix of $g$ <br><br>
let $v, w \in V$ are represented in $\beta$ as
\begin{align*}
\left[ v \right]_{\beta} &= \left[ \begin{array}{c}
                            \alpha_1 \\
                            \alpha_2 \\
                            \vdots \\
                            \alpha_3 
                           \end{array} \right] \quad 
\left[ w \right]_{\beta} = \left[ \begin{array}{c}
                            \sigma_1 \\
                            \sigma_2 \\
                            \vdots \\
                            \sigma_3 
                           \end{array} \right] \\ 
                         v &= \left[ \alpha_1, \alpha_2, \dots \alpha_n \right] 
                              \left[ b_1, b_2, \dots b_n \right] \\
                         v &= \alpha_1 b_1 + \alpha_2 b_2 + \dots + \alpha_n b_n  \tag{1}\\
                         w &= \sigma_1 b_1 + \sigma_2 b_2 + \dots + \sigma_n b_n  \tag{2}
\end{align*} then bilinearity
By Linearity
\begin{align*}
    g(v, w) &= g(\sum_{i=1}^{n} \alpha_i b_i, \sum_{j=1}^{n} \sigma_j b_j) \\
    g(v, w) &= g(\alpha_1 b_1 + \dots + \alpha_n b_n, \sum_{j=1}^{n} \sigma_j b_j) \\
    g(v, w) &= g(\alpha_1 b_1, \sum_{j=1}^{n} \sigma_j b_j) + \dots + g(\alpha_n b_n, \sum_{j=1}^{n} \sigma_j b_j)\\
    g(v, w) &= \sum_{i=1}^{n} g(\alpha_i b_i, \sum_{j=1}^{n} \sigma_j b_j)\\
    g(v, w) &= \sum_{i=1}^{n} g(\alpha_i b_i, \sigma_1 b_1 + \dots + \sigma_n b_n)\\
    g(v, w) &= \sum_{i=1}^{n} g(\alpha_i b_i, \sigma_1 b_1) + \dots + \sum_{i=1}^{n} g(\alpha_i b_i, \sigma_n b_n)\\
    g(v, w) &= \sum_{i=1}^{n} \sum_{j=1}^{n} g(\alpha_i b_i, \sigma_j b_j) \\
    g(v, w) &= \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \sigma_j g(b_i, b_j) \\
\text{From} (1) (2) \\
    g(v, w) &= \left[ v \right]_{\beta}^{T} G \left[ w \right]_{\beta} \\
\end{align*}
Concrete example:<br>
\begin{align*}
    G &= \begin{bmatrix}
        1 & 0\\
        0 & 1
        \end{bmatrix} \\
    g(v, w) &= \left[ x_1, y_1 \right] G \left[ \begin{array}{c}
                                                x_2 \\ 
                                                y_2
                                                 \end{array} \right] \\
    g(v, w) &= \left< v, w \right> = x_1 x_2 + y_1 y_2 \\ \\
    G &= \begin{bmatrix}
        0  & 1\\
        -1 & 0 
        \end{bmatrix} \\
    g(v, w) &= \det \left( \begin{bmatrix}
                      x_1 & x_2 \\
                      y_1 & y_2
                     \end{bmatrix}
                   \right) = x_1 y_2 - x_2 y_1 \\

\end{align*}

A bilinear form is call symmetric form if ... <br>
A bilinear form is call skew symmetric form if ... <br>

matrix bilinear form ... <br>
quadratic form <br>
inner product<br>
cross product<br>
determinant function<br>
http://localhost/pdf/bilinearforms1.pdf

            </div><br>

            <div class="mathdef_box2">
                \[ \textbf{Inner Product} \]
                \[ \text{Positivity} \]
                \[ \langle\mathbf{v}, \mathbf{v}\rangle \geq 0 \]
                \[ \langle \mathbf{v} , \mathbf{v} \rangle = \mathbf{0} \iff \mathbf{v} = \mathbf{0}\]

                \[ \text{Linearity in the first component} \]

                \[ \langle c_{1}\mathbf{v_1} \,, \mathbf{v_2}\rangle = c_{1}\langle \mathbf{v_1}, \mathbf{v_2}\rangle \]
                \[ \langle \mathbf{v_1} + \mathbf{v_2} \,,  \mathbf{v_3} \rangle = \langle \mathbf{v_1} \,, \mathbf{v_3}\rangle  + \langle \mathbf{v_2} \,, \mathbf{v_3}\rangle\]

                \[ \langle c_{1}\mathbf{v_1} + c_{2}\mathbf{v_2}, \mathbf{v_3}\rangle = c_{1}\langle \mathbf{v_1}, \mathbf{v_3}\rangle + c_{2}\langle\mathbf{v_2}, \mathbf{v_3} \rangle \]

                \[ \text{Conjugate Symmetic}\]
                \[ \langle \mathbf{v_1}, \mathbf{v_2} \rangle = \overline{\langle \mathbf{v_2}, \mathbf{v_1} \rangle}\]

                \[ \text{Properties of Inner product}\]
                \[ \langle \mathbf{v_1} \,, \lambda \mathbf{v_2} \rangle =  \overline{\langle \lambda\mathbf{v_2} \,, \mathbf{v_1} \rangle} =  
                    \overline{\lambda} \overline{\langle \mathbf{v_2} \,, \mathbf{v_1} \rangle} = \overline{\lambda} \langle \mathbf{v_1} \,, \mathbf{v_2} \rangle 
                \] 
                <br>

                \[ \textbf{Outer Product} \]
                The outer product $\vec{u} \times \vec{v}$ is equivalent to $u v^{T}$, for instance 
                \[ 
                    u \otimes v = 
                    u v^{T} =
                    \left[ \begin{array}{c}
                    u_1 \\
                    u_2 \\
                    u_3 
                    \end{array} \right] 
                    \left[ \begin{array}{cccc}
                    v_1 & v_2 & v_3 & v_4
                    \end{array} \right] =
                    \begin{bmatrix}
                    u_0 v_1 & u_0 v_2  & u_0 v_3 & u_0 v_4 \\
                    u_2 v_1 & u_2 v_2  & u_2 v_3 & u_2 v_4 \\
                    u_3 v_1 & u_3 v_2  & u_3 v_3 & u_3 v_4 \\
                    \end{bmatrix}
                \]
                \[ \textbf{Matrix Multiplication definited as Outer Product} \]
                \[
                    \begin{bmatrix}
                    a_{11} & a_{12} \\
                    a_{21} & a_{22} \\
                    \end{bmatrix} 
                    \begin{bmatrix}
                    b_{11} & b_{12} \\
                    b_{21} & b_{22} \\
                    \end{bmatrix} = 

                    \begin{bmatrix}
                    a_{11} \\
                    a_{21} 
                    \end{bmatrix} \otimes
                    \begin{bmatrix}
                    b_{11} \\
                    b_{12}
                    \end{bmatrix} 
                    +
                    \begin{bmatrix}
                    a_{12} \\
                    a_{22} 
                    \end{bmatrix} \otimes
                    \begin{bmatrix}
                    b_{21} \\
                    b_{22}
                    \end{bmatrix} 
                    =
                    \begin{bmatrix}
                    a_{11} \\
                    a_{21} 
                    \end{bmatrix} 
                    \begin{bmatrix}
                    b_{11} & b_{12}
                    \end{bmatrix}
                    +
                    \begin{bmatrix}
                    a_{12} \\
                    a_{22} 
                    \end{bmatrix} 
                    \begin{bmatrix}
                    b_{21} & b_{22}
                    \end{bmatrix}
                \]
            </div>



            <div class="mathdef_box2">

           <div class="mytitle">
            Partial Order on Set 
	       </div>                                                                             

            An binary relation $\leq$ defined on Set $A$ is called $\textbf{partial order}$ on set $A$ if following conditions are hold identifically on set $A$
            \begin{align*}
                &a \leq a \\
                &a \leq b \text{ and } b \leq a \Rightarrow a = b \\
                &a \lt b \text{ and } b \lt c \Rightarrow a \lt c \\
                &\text{If in additional, for every } a, b \in A \\
                &a \leq b \text{ or } b \leq a \\
                &\text{ then } \leq \text{ is total order on } A \\
            \end{align*}
            </div>

            <div class="mathdef_box2">
            <div class="mytitle">
            Projection Matrix
            </div>
            \begin{equation}
            \begin{aligned}

            \left< u, v \right>   &=  \|\vec{u}\| \| \vec{v}\| \cos{\phi} \\
            \cos \phi &=  \frac{\left < u , v \right >}{\|u\|\|v\| } \\
            \vec{u} \text{ project on } \vec{v}	 \\	
            \|u\| \cos \phi \frac{v}{\|v\|}   &= \|u\| \frac{ \left < u, v \right>}{\|u\|\|v\|}  \frac{v}{\|v\|}\\	
            \|u\|  \frac{v}{\|v\|} \cos \phi &= \frac{ \left < u, v \right>}{ \left < v, v \right> }  v \\	
            proj_v &= \frac{\left < u, v \right>}{ \left < v, v \right> }  v  =  v \frac{\left< v, u \right>}{\left< v, v \right>}  =  \frac{v (v^{T} u)}{v^T v} = \frac{(v v^{T}) u}{v^{T}v} \\ \\
            \implies proj_v &= \left < u, v \right> v  \quad \text{ where } \|v\| = 1\\
            \implies proj_v &= v v^{T} u  \quad \text{ where } \|v\| = 1\\
            v v^{T} &\text{ is projection matrix from u onto v} \text{ where } \|v\| = 1\\

            \end{aligned}
            \end{equation} 
            </div>

            <div class="mathdef_box2">
            <div class="mytitle">
            Projection Matrix 2            
            </div>                                                                             

            <div style="font-size:140%;">
            Given vectors: $u, v$.<br>
            Project u onto v, let $p = \sigma v$ where $\sigma$ is a scalar <br>
            <br>
            we have<br> 
            \[
                v \perp (p - u)
            \]
            \begin{align*}
            v^{T} (\sigma v - u)  &= 0  \\
            \sigma v^{T} v - v^{T} u &= 0 \\
            \sigma v^{T} v &= v^{T} u  \\
            \sigma &= \frac{v^{T}u}{v^{T}v}  \\
            \sigma v  &= v\frac{v^{T} u}{v^{T}v}   \\
            \sigma v &= \frac{ v v^{T} u } {v^{T} v}  \\
            \sigma v &= \frac{1}{v^{T} v} v v^{T} u  \\
            \sigma v &= \frac{v v^{T}}{v^{T} v} u  \\
            P  &= \frac{v v^{T}}{ v^{T} v } \\
            \end{align*}  

Show $P^{2} = P$, $P$ is idenpotent.<br>
            \begin{align*}
            P^{2} &= \frac{v v^{T}}{v^{T} v} \frac{v v^{T}}{ v^{T} v}  \\
            P^{2} &= \frac{v (v^{T} v) v^{T}}{v^{T} v v^{T} v}  \\
            P^{2} &= \frac{ v v^T} { v^T v} = P 
            \end{align*}  

            $vv^T$ is the projection matrix which projects $u$ onto $v$ where $|v| = 1$<br> 

            Concret Example:
            Let's project $ 
            u = \left[ \begin{array}{c}
            1 \\
            1
            \end{array} \right]$ onto 
            $ v = \left[ \begin{array}{c}
            0 \\
            1
            \end{array} \right]$ <br> 
            $ vv^{T} = 
            \left[ \begin{array}{c}
            0 \\
            1
            \end{array} \right] 
            \left[ \begin{array}{cc}
            0 & 1 
            \end{array} \right] 
            = \begin{bmatrix}
            0 & 0\\
            0 & 1 
            \end{bmatrix} <br> 
            Proj_v = 
            \begin{bmatrix}
            0 & 0\\
            0 & 1 
            \end{bmatrix}
            \left[ \begin{array}{c}
            1 \\
            1
            \end{array} \right] 
            =
            \left[ \begin{array}{c}
            0 \\
            1
            \end{array} \right] 
            $

            <div class="cen">
                <img src="../image/projection_vector.svg" width="80%" height="80%" /><br>  
            </div>
            </div>

	       <div class="mathdef_box2">                                                         

           <div class="mytitle">
	       Inner Product is NOT Bilinear Form over the Complex $\mathbb{C}$                                              
	       Sesquilinear Form and Bilinear Form                                             
	       </div>                                                                             

  What is Bilinear Form: given vector space $V$ and a field $\mathbb{F} = \mathbb{C}$
  $H: V \times V \rightarrow \mathbb{F}$ <br> 
  let $u, v, w \in V, \sigma \in \mathbb{F}$, it satisifed following properties:<br>
   $H(u + v, w) = H(u, w) + H(v, w)$ <br>  
   $H(u, v + w) = H(u, v) + H(u, w)$ <br>  
   $\color{red}{H(\sigma u, v) = \sigma H(u, v)}$ <br>  
   $\color{red}{H(u, \sigma v) = \sigma H(u, v)}$ <br><br>  
  Inner Product: given vector space $V$ and a field $\mathbb{F}$ over the Complex Number $\mathbb{C}$,
  let $u, v, w \in V, \text{ and } \sigma \in \mathbb{F}$ <br>
  $\left<,\right >: V \times V \rightarrow \mathbb{F}$, it satisfied following properties:<br>
  $\left< u + v, w \right > = \left< u, w \right > + \left< v, w \right >$ <br>
  $\color{red}{\left< \sigma u, w \right > = \sigma^{*} \left< u, w \right >} $ <br>
  $\color{red}{\left< u, \sigma v \right > = \sigma \left< u, v \right >} $ <br>
  $\left< u, v \right > = \overline{\left< v, u \right >}$ <br>
  $\left< u, u \right > \geq 0 $ <br>
  $\left< u, u \right > = 0 \Leftrightarrow u = 0 $ <br><br>
\begin{align*}
&\text{Inner Product over the real $\mathbb{R}$ and complex $\mathbb{C}$} \\ 
&\left<\sigma u, v \right > = \overline{\sigma} \left< u, v \right > \quad \because \sigma = \overline{\sigma} \quad \sigma \in \mathbb{R} \\
&\left<\sigma u, v \right > \neq \overline{\sigma} \left< u, v \right > \quad \because \sigma \neq \overline{\sigma} \quad \sigma \in \mathbb{C} \\
\end{align*}
  </div>

	       <div class="mathdef_box2">                                                         
           
           <div class="mytitle">
	       Sesquilinear Form and Bilinear Form                                             
	       </div>                                                                             

           Given $ \left< , \right> : V \times V \rightarrow \mathbb{F}$
           \begin{align*} 
            &\left< u + v, w \right> = \left< u, w \right> + \left< v, w \right> \\
            &\left< u , v + w \right> = \left< u, v \right> + \left< u, w \right> \\
            &\left< \sigma u, v \right> = \overline{\sigma} \left< u, v \right> \\
            &\left< u, \sigma v \right> = \sigma \left< u, v \right> \\
           \end{align*} 
           </div>
           </div>
           </div>

	       <div class="mathdef_box2">                                                         
           
           <div class="mytitle">
	       Jordan Curve Theorem                                 
	       </div>                                                                             

           <div>
           Every simple closed plane curve divides the plane into two components
           </div>
           The above theorem is so obvious.
           </div>

	       <div class="mathdef_box2">                                                         
           <div class="mytitle">
	       Collinear points     
	       </div>                                                                             

           <div>
           Three or more points are said to be collinear if they lie on a single straigh line 
           </div>
           <div class="cen">
           <img src="../image/threepts_collinear.svg" width="100%" height="100%" /><br>  
           </div>
           </div>

	   <div class="mathdef_box2">                                                         
           <div class="mytitle">
           Euclid's three postulates  
           </div>
           1. A straigh line segment can be drawn joining any two points<br>
           2. A straigh line segment can be extended indefinitely to a straigh line<br>
           3. Given any straigh line segment, a straigh line segment can be drawn having the segment as radius and one endpoint as center<br>
           <div class="cen">
           <img src="../image/euclid_three_postulates.svg" width="100%" height="100%" /><br>  
           </div>
           </div>

           <div class="mathdef_box2">                                                                                                           
           <div class="mytitle">                                                                                                                
           Cross Product                                                                                                             
           </div>
           Given vectors $u = (b_1, b_2, b_3), v = (c_1, c_2, c_3)$, the cross product of $u, v$ is the following: <br>

        \begin{equation}
        \begin{aligned}
        u \times v &= w = \begin{bmatrix} 
            i & j & k \\ 
            b_1 & b_2 & b_3 \\ 
            c_1 & c_2 & c_3  
            \end{bmatrix}  \\
        \det{A}&=i (-1)^{1 + 1} 
                \begin{vmatrix} 
                b_2 & b_3 \\ 
                c_2 & c_3  
                \end{vmatrix} + j (-1)^{2 + 1} 
                \begin{vmatrix} 
                a_2 & a_3 \\
                c_2 & c_3  
                \end{vmatrix} + k (-1)^{3 + 1} 
                \begin{vmatrix}
                a_2 & a_3 \\
                b_2 & b_3 \\
                \end{vmatrix}
        \end{aligned}
        \end{equation} 
	   
	The direction of cross product can be determinated by the Right Hand Rule <br>
	The magnitude can be computed as following: <br>
	\[  u \times v = \|u\| \|v\| \sin{\alpha} \vec{n} \quad \text{ where } \vec{n} \text{ is the unit vector that is perpendicular to the plane which contains } \vec{u} \vec{v} \]  <br>
        The magnitude of cross product is just the volumn of parallelepiped $\vec{u}, \vec{v}, \vec{w}$<br>
		   
           <div class="cen">                                                                
           <img src="../image/cross_product.svg" width="100%" height="100%" /><br>	
           </div>                                                                           	
        </div>
	   <a name="prove_determinant"><div class="mathdef_box2">                                                         
           <div class="mytitle">
           Prove the determinant of matrix is equal to the determinant of the transpose matrix  
           </div>
       Proof 1:<br>
       Given an n by n matrix $m$, prove $\det m = \det m^{T}$ <br>
       From the $QR$ decomposition, any square matrix can be decomposed into $Q$ is upper triangle matrix and
       $R$ is orthgonal matrix
       
       \begin{equation}
       \begin{aligned}
       m &= QR \\
       m^{T} &= (QR)^{T} \\
       \det m^{T} &= \det (QR)^{T} \\
       \det m^{T} &= \det R^{T} \det Q^{T} \\
       \det m^{T} &= \det R^{-1} \det Q  \quad \text{ where } \det Q = \det Q^{T} \because Q \text{ is upper triangle matrix} \\
       R^{T} &= R^{-1} \because R \text{ is othgonal matrix} \\ 
       \det m^{T} &= \frac{\det Q}{\det R} \\
       \det m^{T} &= \det Q  \quad \text{ where } \det R = 1 \because R \text{ is orthgonal matrix} \\ 
       \Rightarrow \det m &= \det (QR) = \det R \det Q = \det Q = \det m^{T} \quad \square \\ 
       \end{aligned}
       \end{equation}
       <br>
       Proof 2: Using co-factor expansion and induction on the dimension $n$.<br>
       coming soon
       </div></a>

	   <div class="mathdef_box2">                                                         
           <div class="mytitle">
           Unit Circle and Group Law 
           </div>
           <div class="cen">                                                                
           <img src="../image/circle_group_law.svg" width="100%" height="100%" /><br>	
           </div>                                                                           	
         Draw line parallel to $\overline{P_1 P_2}$ and pass $O$
        \[ 
            P_1 \oplus P_2 = P_3
        \]
       </div>

</body>
